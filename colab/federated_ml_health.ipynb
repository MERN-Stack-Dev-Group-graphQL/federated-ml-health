{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "federated-ml-health",
      "provenance": [],
      "collapsed_sections": [
        "c7zQveWs6Cxx"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7zQveWs6Cxx"
      },
      "source": [
        "# federated-ml-health\n",
        "\n",
        "Copyright 2020 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASoXJP3e4XX-"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This colab implements side-by-side comparison of models and their interpretations trained in a centralized machine learning (a.k.a. classical) way vs. the federated (distributed) way. Specifically, we concentrate on binary inference problems and evaluate several approaches: \n",
        "\n",
        "1.   regression models trained in a traditional way using various optimizers on centralized datasets\n",
        "2.   equivalent models expressed in tensorflow and trained using available optimizers therein\n",
        "3.   equivalent models trained in tensorflow_federated where each device keeps its data local and private\n",
        "4.   all of the above with differential privacy added"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tF3IEWxX2CKP"
      },
      "source": [
        "# To install TFF and dependencies\n",
        "!pip uninstall --quiet --yes tensorflow-datasets tensorflow-metadata\n",
        "!pip install --quiet --upgrade nest_asyncio\n",
        "!pip install --quiet --upgrade tensorflow_federated_nightly\n",
        "!pip install --quiet --upgrade tensorflow_privacy\n",
        "\n",
        "# Note: It is safe to ignore the `ERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.` as that is an artifact of preinstalled packages in the default colab runtime.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMB_hsVf4w0D"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Data initialization\n",
        "\n",
        "Using a public diabetes multi-factorial dataset for quick eval on a specific well-studied problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leOpdF3N1P6F"
      },
      "source": [
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import nest_asyncio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import tempfile\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "from collections import defaultdict \n",
        "from io import StringIO\n",
        "from keras.utils import to_categorical\n",
        "from matplotlib.pyplot import figure\n",
        "from numpy import loadtxt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "nest_asyncio.apply()\n",
        "tff.framework.set_default_context(tff.backends.native.create_thread_debugging_execution_context(clients_per_thread=50))\n",
        "np.random.seed(10)\n",
        "tf.random.set_seed(10)\n",
        "\n",
        "f = tempfile.NamedTemporaryFile(mode=\"w\")\n",
        "\n",
        "def get_diabetes_dataset():\n",
        "  # Download data from https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.names and paste to return statement here.\n",
        "  return \"\"\"\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "def get_diabetes_labels():\n",
        "  return [\n",
        "      \"intercept\", \"Number of times pregnant\", \"Plasma glucose concentration\",\n",
        "      \"Diastolic blood pressure\", \"Triceps skin fold thickness\",\n",
        "      \"2-Hour serum insulin (mu U/ml)\", \"BMI\", \"Diabetes pedigree function\",\n",
        "      \"Age (years)\", \"Diabetic?\"\n",
        "  ]\n",
        "\n",
        "dataset = get_diabetes_dataset()\n",
        "labels = get_diabetes_labels()\n",
        "\n",
        "f.write(dataset)\n",
        "dataset = loadtxt(f.name, delimiter=\",\", skiprows=1)\n",
        "num_col = np.size(dataset, 1)\n",
        "X = dataset[:, 0:num_col-1]\n",
        "y = dataset[:, num_col-1]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X)\n",
        "X = scaler.transform(X)\n",
        "\n",
        "D = np.column_stack((X, y))\n",
        "ds = pd.DataFrame(\n",
        "    data=D,\n",
        "    columns=labels[1:])\n",
        "\n",
        "ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gmfdu_98F5rb"
      },
      "source": [
        "# Test centralized predictive power of all models and training regimes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-qh2h9qF9sV"
      },
      "source": [
        "TRAIN_PROPORTION = 0.8\n",
        "NUM_FEATURES = np.size(X, 1)\n",
        "NUM_ROUNDS = 12\n",
        "n_train = round(TRAIN_PROPORTION * np.size(X, 0))\n",
        "NUM_CLIENTS = n_train\n",
        "NUM_PARTICIPATING_PER_ROUND = round(NUM_CLIENTS/3)\n",
        "\n",
        "data_train = X[:n_train]\n",
        "labels_train =  y[:n_train]\n",
        "data_test = X[n_train:]\n",
        "labels_test =  y[n_train:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mR9evztMh702"
      },
      "source": [
        "# sklearn regression\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frzfmadVh4zx"
      },
      "source": [
        "sk_model = LogisticRegression(random_state=0, solver='liblinear').fit(data_train, labels_train)\n",
        "proba = sk_model.predict_proba(data_test)\n",
        "labels_proba = proba[:,1]\n",
        "fpr_skl_liblinear, tpr_skl_liblinear, threshold_skl_liblinear = sklearn.metrics.roc_curve(labels_test, labels_proba)\n",
        "roc_auc_skl_liblinear = sklearn.metrics.auc(fpr_skl_liblinear, tpr_skl_liblinear)\n",
        "print(roc_auc_skl_liblinear)\n",
        "\n",
        "sk_model = LogisticRegression(random_state=0, solver='sag').fit(data_train, labels_train)\n",
        "proba = sk_model.predict_proba(data_test)\n",
        "labels_proba = proba[:,1]\n",
        "fpr_skl_sag, tpr_skl_sag, threshold_skl_sag = sklearn.metrics.roc_curve(labels_test, labels_proba)\n",
        "roc_auc_skl_sag = sklearn.metrics.auc(fpr_skl_sag, tpr_skl_sag)\n",
        "print(roc_auc_skl_sag)\n",
        "\n",
        "sk_model = LogisticRegression(random_state=0, solver='lbfgs').fit(data_train, labels_train)\n",
        "proba = sk_model.predict_proba(data_test)\n",
        "labels_proba = proba[:,1]\n",
        "fpr_skl, tpr_skl, threshold_skl = sklearn.metrics.roc_curve(labels_test, labels_proba)\n",
        "roc_auc_skl = sklearn.metrics.auc(fpr_skl, tpr_skl)\n",
        "print(roc_auc_skl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv1I3oDbiBlx"
      },
      "source": [
        "# TF regression\n",
        "\n",
        "Comparing weights obtained with common regression weight learning method often used in literature to those obtained with general neural network optimization (expressing regression as a special instance of neural architecture with one layer).\n",
        "\n",
        "Adam optimization method is used to mimic the sklearn solver as close as possible (leveraging second derivatives of gradient)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3VJqYAuiD7t"
      },
      "source": [
        "dataset_train = tf.data.Dataset.from_tensor_slices((data_train, labels_train)).batch(n_train)\n",
        "dataset_test = tf.data.Dataset.from_tensor_slices((data_test, labels_test)).batch(n_train)\n",
        "\n",
        "def create_keras_model():\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(\n",
        "          1,\n",
        "          activation='sigmoid',\n",
        "          input_shape=(NUM_FEATURES,),\n",
        "          kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "      )\n",
        "  ])\n",
        "\n",
        "def create_keras_model_deeper():\n",
        "  initializer = tf.keras.initializers.GlorotNormal(seed=10)\n",
        "  m = tf.keras.models.Sequential()\n",
        "  m.add(tf.keras.Input(shape=(NUM_FEATURES,)))\n",
        "  m.add(tf.keras.layers.Dense(6, activation='sigmoid', kernel_initializer=initializer))\n",
        "  m.add(tf.keras.layers.Dense(3, activation='sigmoid', kernel_initializer=initializer))\n",
        "  m.add(tf.keras.layers.Dense(1, activation='sigmoid', kernel_initializer=initializer, kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.0001, l2=0.01)))\n",
        "  return m\n",
        "  \n",
        "\n",
        "tf_model = create_keras_model()\n",
        "tf_model.compile(\n",
        "              optimizer=tf.keras.optimizers.Nadam(learning_rate=0.5),   \n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=[\n",
        "                       tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "                       tf.keras.metrics.AUC(name='auc'),\n",
        "                       ]\n",
        "              )\n",
        "tf_model.fit(dataset_train, validation_data=dataset_test, epochs=NUM_ROUNDS, batch_size=NUM_PARTICIPATING_PER_ROUND, verbose=1, use_multiprocessing=True)\n",
        "\n",
        "labels_proba = tf_model.predict(dataset_test)\n",
        "fpr_tf, tpr_tf, threshold = sklearn.metrics.roc_curve(labels_test, labels_proba)\n",
        "roc_auc_tf = sklearn.metrics.auc(fpr_tf, tpr_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7hRe3fwiM54"
      },
      "source": [
        "# TF Federated regression\n",
        "\n",
        "Optimizing the same model architecture as above in the TF case, but here trained in a federated (distributed) way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjbbq1mkiUf2"
      },
      "source": [
        "def create_client_dataset(data, labels):\n",
        "  client_ids = range(len(data))\n",
        "\n",
        "  def create_dataset_fn(client_id):\n",
        "    return tf.data.Dataset.from_tensor_slices((data[client_id:client_id+1,:], labels[client_id:client_id+1]))\n",
        "\n",
        "  return tff.simulation.ClientData.from_clients_and_fn(\n",
        "      client_ids=client_ids,\n",
        "      create_tf_dataset_for_client_fn=create_dataset_fn)\n",
        "  \n",
        "def preprocess(dataset):\n",
        "  return dataset.repeat(1).batch(1)\n",
        "\n",
        "def make_federated_data(client_data, client_ids):\n",
        "  return [\n",
        "      preprocess(client_data.create_tf_dataset_for_client(x))\n",
        "      for x in client_ids\n",
        "  ]\n",
        "\n",
        "client_dataset_train = create_client_dataset(data_train, labels_train)\n",
        "client_dataset_test = create_client_dataset(data_test, labels_test)\n",
        "\n",
        "preprocessed_example_dataset = preprocess(client_dataset_train.create_tf_dataset_for_client(client_dataset_train.client_ids[0]))\n",
        "print(preprocessed_example_dataset.element_spec)\n",
        "\n",
        "def model_fn():\n",
        "  keras_model = create_keras_model()\n",
        "  return tff.learning.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=preprocessed_example_dataset.element_spec,\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "               tf.keras.metrics.AUC(name='auc')])\n",
        "  \n",
        "# Create TFF interative process.\n",
        "iterative_process = tff.learning.build_federated_averaging_process(\n",
        "    model_fn,\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.Nadam(learning_rate=0.5),\n",
        "    use_experimental_simulation_loop = True\n",
        ")\n",
        "\n",
        "state = iterative_process.initialize()\n",
        "tff_model = create_keras_model()\n",
        "tff_auc = defaultdict(lambda:0)\n",
        "\n",
        "# Test various sizes of subsets of eligible devices participating in each round.\n",
        "for participation in list(range(1, NUM_CLIENTS, 100)) + [NUM_CLIENTS]:\n",
        "  for round_num in range(NUM_ROUNDS):\n",
        "    federated_train_data = make_federated_data(client_dataset_train, np.random.choice(range(NUM_CLIENTS), size=participation, replace=False))\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "    print(participation, round_num, str(metrics))\n",
        "    state.model.assign_weights_to(tff_model)\n",
        "    labels_proba = tff_model.predict(dataset_test)\n",
        "    fpr, tpr, threshold = sklearn.metrics.roc_curve(labels_test, labels_proba)\n",
        "    test_loss = tf.keras.losses.binary_crossentropy(labels_test, np.reshape(labels_proba, [-1]))\n",
        "    print('validation auc={}, loss={}'.format(sklearn.metrics.auc(fpr, tpr), test_loss))\n",
        "    tff_auc[100*participation/float(NUM_CLIENTS)] = max(tff_auc[100*participation/float(NUM_CLIENTS)], sklearn.metrics.auc(fpr, tpr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFON97SnQVK5"
      },
      "source": [
        "state.model.assign_weights_to(tff_model)\n",
        "labels_proba = tff_model.predict(dataset_test)\n",
        "fpr_tff_sgd, tpr_tff_sgd, threshold_tff_sgd = sklearn.metrics.roc_curve(labels_test, labels_proba)\n",
        "roc_auc_tff_sgd = sklearn.metrics.auc(fpr_tff_sgd, tpr_tff_sgd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP5EHjizIWgR"
      },
      "source": [
        "# TFF regression with DP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eryEtyjkIO29"
      },
      "source": [
        "print(preprocessed_example_dataset.element_spec)\n",
        "\n",
        "tff_dp_auc = defaultdict(lambda:0)\n",
        "\n",
        "# Test various sizes of subsets of eligible devices participating in each round.\n",
        "for participation in list(range(1, NUM_CLIENTS, 100)) + [NUM_CLIENTS]:\n",
        "  # DP process depends on participation rate to select noise scale.\n",
        "  dp_query = tff.utils.build_dp_query(\n",
        "      clip=0.1,\n",
        "      noise_multiplier=29.3,\n",
        "      expected_total_weight=participation,\n",
        "      adaptive_clip_learning_rate=0,\n",
        "      target_unclipped_quantile=0.5,\n",
        "      clipped_count_budget_allocation=0.05,\n",
        "      expected_clients_per_round=participation,\n",
        "      per_vector_clipping=True,\n",
        "      geometric_clip_update=True,\n",
        "      model=model_fn())\n",
        "\n",
        "  weights_type = tff.learning.framework.weights_type_from_model(model_fn)\n",
        "  dp_aggregation_process = tff.utils.build_dp_aggregate_process(\n",
        "      weights_type.trainable, dp_query)\n",
        "\n",
        "  # Create TFF interative process.\n",
        "  iterative_process = tff.learning.build_federated_averaging_process(\n",
        "      model_fn,\n",
        "      client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),\n",
        "      server_optimizer_fn=lambda: tf.keras.optimizers.Nadam(learning_rate=0.5),\n",
        "      use_experimental_simulation_loop = True,\n",
        "      aggregation_process=dp_aggregation_process,\n",
        "  )\n",
        "\n",
        "  state = iterative_process.initialize()\n",
        "\n",
        "  for round_num in range(10):\n",
        "    federated_train_data = make_federated_data(client_dataset_train, np.random.choice(range(NUM_CLIENTS), size=participation, replace=False))\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "    print(participation, round_num, str(metrics))\n",
        "    tff_dp_model = create_keras_model() # reassigning weights to the same model was causing a bug\n",
        "    state.model.assign_weights_to(tff_dp_model)\n",
        "    labels_proba = tff_dp_model.predict(dataset_test)\n",
        "    fpr, tpr, threshold = sklearn.metrics.roc_curve(labels_test, labels_proba)\n",
        "    test_loss = tf.keras.losses.binary_crossentropy(labels_test, np.reshape(labels_proba, [-1]))\n",
        "    print('validation auc={}, loss={}'.format(sklearn.metrics.auc(fpr, tpr), test_loss))\n",
        "    tff_dp_auc[100*participation/float(NUM_CLIENTS)] = max(tff_dp_auc[100*participation/float(NUM_CLIENTS)], sklearn.metrics.auc(fpr, tpr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seWkZog6IO3A"
      },
      "source": [
        "state.model.assign_weights_to(tff_dp_model)\n",
        "labels_proba = tff_dp_model.predict(dataset_test)\n",
        "fpr_tff_dp_sgd, tpr_tff_dp_sgd, threshold_tff_dp_sgd = sklearn.metrics.roc_curve(labels_test, labels_proba)\n",
        "roc_auc_tff_dp_sgd = sklearn.metrics.auc(fpr_tff_dp_sgd, tpr_tff_dp_sgd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NupayoOhLkaV"
      },
      "source": [
        "# TF regression with differential privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxpiZZ6oVx0a"
      },
      "source": [
        "This is needed to make TF Privacy work in TF 2.0+. Specifically, wrapping the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOlBNhMnVjDm"
      },
      "source": [
        "from absl import logging\n",
        "import collections\n",
        "\n",
        "from tensorflow_privacy.privacy.analysis import privacy_ledger\n",
        "from tensorflow_privacy.privacy.dp_query import gaussian_query\n",
        "\n",
        "def make_optimizer_class(cls):\n",
        "  \"\"\"Constructs a DP optimizer class from an existing one.\"\"\"\n",
        "  parent_code = tf.compat.v1.train.Optimizer.compute_gradients.__code__\n",
        "  child_code = cls.compute_gradients.__code__\n",
        "  GATE_OP = tf.compat.v1.train.Optimizer.GATE_OP  # pylint: disable=invalid-name\n",
        "  if child_code is not parent_code:\n",
        "    logging.warning(\n",
        "        'WARNING: Calling make_optimizer_class() on class %s that overrides '\n",
        "        'method compute_gradients(). Check to ensure that '\n",
        "        'make_optimizer_class() does not interfere with overridden version.',\n",
        "        cls.__name__)\n",
        "\n",
        "  class DPOptimizerClass(cls):\n",
        "    \"\"\"Differentially private subclass of given class cls.\"\"\"\n",
        "\n",
        "    _GlobalState = collections.namedtuple(\n",
        "      '_GlobalState', ['l2_norm_clip', 'stddev'])\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        dp_sum_query,\n",
        "        num_microbatches=None,\n",
        "        unroll_microbatches=False,\n",
        "        *args,  # pylint: disable=keyword-arg-before-vararg, g-doc-args\n",
        "        **kwargs):\n",
        "      \"\"\"Initialize the DPOptimizerClass.\n",
        "\n",
        "      Args:\n",
        "        dp_sum_query: DPQuery object, specifying differential privacy\n",
        "          mechanism to use.\n",
        "        num_microbatches: How many microbatches into which the minibatch is\n",
        "          split. If None, will default to the size of the minibatch, and\n",
        "          per-example gradients will be computed.\n",
        "        unroll_microbatches: If true, processes microbatches within a Python\n",
        "          loop instead of a tf.while_loop. Can be used if using a tf.while_loop\n",
        "          raises an exception.\n",
        "      \"\"\"\n",
        "      super(DPOptimizerClass, self).__init__(*args, **kwargs)\n",
        "      self._dp_sum_query = dp_sum_query\n",
        "      self._num_microbatches = num_microbatches\n",
        "      self._global_state = self._dp_sum_query.initial_global_state()\n",
        "      self._unroll_microbatches = unroll_microbatches\n",
        "\n",
        "    def compute_gradients(self,\n",
        "                          loss,\n",
        "                          var_list,\n",
        "                          gate_gradients=GATE_OP,\n",
        "                          aggregation_method=None,\n",
        "                          colocate_gradients_with_ops=False,\n",
        "                          grad_loss=None,\n",
        "                          gradient_tape=None,\n",
        "                          curr_noise_mult=0,\n",
        "                          curr_norm_clip=1):\n",
        "\n",
        "      self._dp_sum_query = gaussian_query.GaussianSumQuery(curr_norm_clip, \n",
        "                                                           curr_norm_clip*curr_noise_mult)\n",
        "      self._global_state = self._dp_sum_query.make_global_state(curr_norm_clip, \n",
        "                                                                curr_norm_clip*curr_noise_mult)\n",
        "      \n",
        "\n",
        "      # TF is running in Eager mode, check we received a vanilla tape.\n",
        "      if not gradient_tape:\n",
        "        raise ValueError('When in Eager mode, a tape needs to be passed.')\n",
        "\n",
        "      vector_loss = loss()\n",
        "      if self._num_microbatches is None:\n",
        "        self._num_microbatches = tf.shape(input=vector_loss)[0]\n",
        "      sample_state = self._dp_sum_query.initial_sample_state(var_list)\n",
        "      microbatches_losses = tf.reshape(vector_loss, [self._num_microbatches, -1])\n",
        "      sample_params = (self._dp_sum_query.derive_sample_params(self._global_state))\n",
        "\n",
        "      def process_microbatch(i, sample_state):\n",
        "        \"\"\"Process one microbatch (record) with privacy helper.\"\"\"\n",
        "        microbatch_loss = tf.reduce_mean(input_tensor=tf.gather(microbatches_losses, [i]))\n",
        "        grads = gradient_tape.gradient(microbatch_loss, var_list)\n",
        "        sample_state = self._dp_sum_query.accumulate_record(sample_params, sample_state, grads)\n",
        "        return sample_state\n",
        "    \n",
        "      for idx in range(self._num_microbatches):\n",
        "        sample_state = process_microbatch(idx, sample_state)\n",
        "\n",
        "      if curr_noise_mult > 0:\n",
        "        grad_sums, self._global_state = (self._dp_sum_query.get_noised_result(sample_state, self._global_state))\n",
        "      else:\n",
        "        grad_sums = sample_state\n",
        "\n",
        "      def normalize(v):\n",
        "        return v / tf.cast(self._num_microbatches, tf.float32)\n",
        "\n",
        "      final_grads = tf.nest.map_structure(normalize, grad_sums)\n",
        "      grads_and_vars = final_grads\n",
        "    \n",
        "      return grads_and_vars\n",
        "\n",
        "  return DPOptimizerClass\n",
        "\n",
        "\n",
        "def make_gaussian_optimizer_class(cls):\n",
        "  \"\"\"Constructs a DP optimizer with Gaussian averaging of updates.\"\"\"\n",
        "\n",
        "  class DPGaussianOptimizerClass(make_optimizer_class(cls)):\n",
        "    \"\"\"DP subclass of given class cls using Gaussian averaging.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        l2_norm_clip,\n",
        "        noise_multiplier,\n",
        "        num_microbatches=None,\n",
        "        ledger=None,\n",
        "        unroll_microbatches=False,\n",
        "        *args,  # pylint: disable=keyword-arg-before-vararg\n",
        "        **kwargs):\n",
        "      dp_sum_query = gaussian_query.GaussianSumQuery(\n",
        "          l2_norm_clip, l2_norm_clip * noise_multiplier)\n",
        "\n",
        "      if ledger:\n",
        "        dp_sum_query = privacy_ledger.QueryWithLedger(dp_sum_query,\n",
        "                                                      ledger=ledger)\n",
        "\n",
        "      super(DPGaussianOptimizerClass, self).__init__(\n",
        "          dp_sum_query,\n",
        "          num_microbatches,\n",
        "          unroll_microbatches,\n",
        "          *args,\n",
        "          **kwargs)\n",
        "\n",
        "    @property\n",
        "    def ledger(self):\n",
        "      return self._dp_sum_query.ledger\n",
        "\n",
        "  return DPGaussianOptimizerClass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlG2Kv2WLnaa"
      },
      "source": [
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPAdamGaussianOptimizer\n",
        "\n",
        "l2_norm_clip = 0.1\n",
        "noise_multiplier = 59.3\n",
        "num_microbatches = 1\n",
        "learning_rate = 0.5\n",
        "\n",
        "GradientDescentOptimizer = tf.compat.v1.train.GradientDescentOptimizer\n",
        "DPGradientDescentGaussianOptimizer_NEW = make_gaussian_optimizer_class(GradientDescentOptimizer)\n",
        "\n",
        "dp_optimizer = DPGradientDescentGaussianOptimizer_NEW(\n",
        "    l2_norm_clip=l2_norm_clip,\n",
        "    noise_multiplier=noise_multiplier,\n",
        "    num_microbatches=num_microbatches,\n",
        "    learning_rate=learning_rate)\n",
        "\n",
        "dp_loss = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "tf_dp_model = create_keras_model()\n",
        "tf_dp_model.compile(\n",
        "              optimizer=dp_optimizer,   \n",
        "              loss=dp_loss,\n",
        "              metrics=[\n",
        "                       tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "                       tf.keras.metrics.AUC(name='auc'),\n",
        "                       ]\n",
        "              )\n",
        "tf_dp_model.fit(dataset_train, validation_data=dataset_test, epochs=NUM_ROUNDS, batch_size=NUM_PARTICIPATING_PER_ROUND, verbose=1, use_multiprocessing=True)\n",
        "\n",
        "labels_proba = tf_dp_model.predict(dataset_test)\n",
        "fpr_tf_dp, tpr_tf_dp, threshold = sklearn.metrics.roc_curve(labels_test, labels_proba)\n",
        "roc_auc_tf_dp = sklearn.metrics.auc(fpr_tf_dp, tpr_tf_dp)\n",
        "\n",
        "compute_dp_sgd_privacy.compute_dp_sgd_privacy(n=n_train, batch_size=NUM_PARTICIPATING_PER_ROUND, noise_multiplier=noise_multiplier, epochs=NUM_ROUNDS, delta=1/n_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLVNvcW_iZ4k"
      },
      "source": [
        "# Predictive power comparison of all models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHWXzjiRic62"
      },
      "source": [
        "figure(num=None, figsize=(8, 6), dpi=150, facecolor='w', edgecolor='k')\n",
        "plt.title('ROC')\n",
        "plt.plot(fpr_skl_liblinear, tpr_skl_liblinear, label = 'Sklearn LR liblinear AUC = %0.3f' % roc_auc_skl_liblinear)\n",
        "plt.plot(fpr_skl_sag, tpr_skl_sag, label = 'Sklearn LR SAG AUC = %0.3f' % roc_auc_skl_sag)\n",
        "plt.plot(fpr_tf, tpr_tf, label = 'TF Centralized LR AUC = %0.3f' % roc_auc_tf)\n",
        "plt.plot(fpr_tf_dp, tpr_tf_dp, label = 'TF Centralized LR with DP AUC = %0.3f' % roc_auc_tf_dp)\n",
        "plt.plot(fpr_tff_sgd, tpr_tff_sgd, label = 'TF Federated LR SGDM AUC = %0.3f' % roc_auc_tff_sgd)\n",
        "plt.plot(fpr_tff_dp_sgd, tpr_tff_dp_sgd, label = 'TF Federated LR with DP SGDM AUC = %0.3f' % roc_auc_tff_dp_sgd)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7gLxKazLqq3"
      },
      "source": [
        "figure(num=None, figsize=(8, 6), dpi=150, facecolor='w', edgecolor='k')\n",
        "plt.title('AUC as a function of participation of devices')\n",
        "s = sorted(tff_auc.items(), key = lambda kv:(kv[0], kv[1]))\n",
        "x_data = [o[0] for o in s]\n",
        "y_data = [o[1] for o in s]\n",
        "plt.plot(x_data, y_data, label = 'TF Federated LR SGDM')\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.ylabel('AUC achieved')\n",
        "plt.xlabel('% of participating devices (random subset in each round)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAN3DxsCitGO"
      },
      "source": [
        "# Model parameters comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k-EFp7Viwen"
      },
      "source": [
        "output_sklearn = sk_model.coef_[0]\n",
        "output_sklearn = np.insert(output_sklearn, 0, sk_model.intercept_)\n",
        "\n",
        "output_tf = tf_model.get_weights()[0]\n",
        "output_tf = np.insert(output_tf, 0, tf_model.get_weights()[1])\n",
        "\n",
        "output_tff = tff_model.get_weights()[0]\n",
        "output_tff = np.insert(output_tff, 0, tff_model.get_weights()[1])\n",
        "\n",
        "print(output_sklearn)\n",
        "print(output_tf)\n",
        "print(output_tff)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JvdqOncmyyR"
      },
      "source": [
        "# Performance grid eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBeJFq7bmxrv"
      },
      "source": [
        "import time\n",
        "from sklearn import datasets\n",
        "\n",
        "def create_keras_model_benchmark(input_dim):\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(\n",
        "          1,\n",
        "          activation='sigmoid',\n",
        "          input_shape=(input_dim,),\n",
        "          kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "      )\n",
        "  ])\n",
        "\n",
        "def model_builder(input_dim, input_spec):\n",
        "  keras_model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(\n",
        "          1,\n",
        "          activation='sigmoid',\n",
        "          input_shape=(input_dim,),\n",
        "          kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "      )\n",
        "  ])\n",
        "  return tff.learning.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=input_spec,\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "               tf.keras.metrics.AUC(name='auc')])\n",
        "\n",
        "\n",
        "# Synthetic dataset\n",
        "MAX_NUM_EXAMPLES = 500\n",
        "INCREMENT_EXAMPLES = 100\n",
        "MAX_NUM_FEATURES = 200\n",
        "INCREMENT_FEATURES = 50\n",
        "\n",
        "eval_data = np.zeros((round(MAX_NUM_EXAMPLES/INCREMENT_EXAMPLES), round(MAX_NUM_FEATURES/INCREMENT_FEATURES))) \n",
        "\n",
        "for e, examples_size in enumerate(list(range(INCREMENT_EXAMPLES, MAX_NUM_EXAMPLES+1, INCREMENT_EXAMPLES))):\n",
        "  for f, features_size in enumerate(list(range(INCREMENT_FEATURES, MAX_NUM_FEATURES+1, INCREMENT_FEATURES))):\n",
        "    dataset, labels = sklearn.datasets.make_classification(n_samples=examples_size, n_features=features_size, n_classes=2, weights=[0.75])\n",
        "    n_train = round(TRAIN_PROPORTION * np.size(dataset, 0))\n",
        "    data_train = dataset[:n_train]\n",
        "    labels_train =  labels[:n_train]\n",
        "    data_test = dataset[n_train:]\n",
        "    labels_test =  labels[n_train:]\n",
        "\n",
        "    client_dataset_train = create_client_dataset(data_train, labels_train)\n",
        "    client_dataset_test = create_client_dataset(data_test, labels_test)\n",
        "    preprocessed_example_dataset_bench = preprocess(client_dataset_train.create_tf_dataset_for_client(client_dataset_train.client_ids[0]))\n",
        "\n",
        "    # Create TFF interative process.\n",
        "    iterative_process = tff.learning.build_federated_averaging_process(\n",
        "        model_fn=lambda: model_builder(input_dim=features_size, input_spec=preprocessed_example_dataset_bench.element_spec),\n",
        "        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),\n",
        "        server_optimizer_fn=lambda: tf.keras.optimizers.Nadam(learning_rate=0.5),\n",
        "        use_experimental_simulation_loop = True\n",
        "    )\n",
        "\n",
        "    state = iterative_process.initialize()\n",
        "    tff_model_bench = create_keras_model_benchmark(features_size)\n",
        "    federated_train_data = make_federated_data(client_dataset_train, range(n_train))\n",
        "\n",
        "    start_t = time.process_time()\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "    elapsed_time = time.process_time() - start_t\n",
        "    eval_data[e][f] = elapsed_time\n",
        "\n",
        "    print(examples_size, features_size, elapsed_time, str(metrics))\n",
        "    state.model.assign_weights_to(tff_model_bench)\n",
        "    labels_proba = tff_model_bench.predict(data_test)\n",
        "    fpr, tpr, threshold = sklearn.metrics.roc_curve(labels_test, labels_proba)\n",
        "    test_loss = tf.keras.losses.binary_crossentropy(labels_test, np.reshape(labels_proba, [-1]))\n",
        "    print('validation auc={}, loss={}'.format(sklearn.metrics.auc(fpr, tpr), test_loss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1PSNw8gnU2q"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "x_axis = list(range(INCREMENT_FEATURES, MAX_NUM_FEATURES+1, INCREMENT_FEATURES))\n",
        "y_axis = list(range(INCREMENT_EXAMPLES, MAX_NUM_EXAMPLES+1, INCREMENT_EXAMPLES))\n",
        "plt.figure(figsize=(6, 6), dpi=150)\n",
        "plt.title('Runtime [s] as a function of number of clients and features')\n",
        "color_map = plt.imshow(eval_data)\n",
        "color_map.set_cmap(\"Blues_r\")\n",
        "plt.colorbar()\n",
        "plt.ylabel('# Examples')\n",
        "plt.xlabel('# Features')\n",
        "plt.yticks(ticks=range(len(y_axis)), labels=y_axis)\n",
        "plt.xticks(ticks=range(len(x_axis)), labels=x_axis)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GJE1inPhtEo"
      },
      "source": [
        "# Runtime as a function of number of clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWnzQpgFhiY_"
      },
      "source": [
        "import time\n",
        "from sklearn import datasets\n",
        "\n",
        "\n",
        "def create_keras_model_benchmark(input_dim):\n",
        "  return tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(\n",
        "          1,\n",
        "          activation='sigmoid',\n",
        "          input_shape=(input_dim,),\n",
        "          kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "      )\n",
        "  ])\n",
        "\n",
        "def model_builder(input_dim, input_spec):\n",
        "  keras_model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Dense(\n",
        "          1,\n",
        "          activation='sigmoid',\n",
        "          input_shape=(input_dim,),\n",
        "          kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
        "      )\n",
        "  ])\n",
        "  return tff.learning.from_keras_model(\n",
        "      keras_model,\n",
        "      input_spec=input_spec,\n",
        "      loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "      metrics=[tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "               tf.keras.metrics.AUC(name='auc')])\n",
        "\n",
        "\n",
        "# Synthetic dataset\n",
        "MAX_NUM_EXAMPLES = 10000\n",
        "INCREMENT_EXAMPLES = 1000\n",
        "MAX_NUM_FEATURES = 60\n",
        "INCREMENT_FEATURES = 60\n",
        "\n",
        "eval_data = np.zeros((round(MAX_NUM_EXAMPLES/INCREMENT_EXAMPLES), round(MAX_NUM_FEATURES/INCREMENT_FEATURES))) \n",
        "\n",
        "for e, examples_size in enumerate(list(range(INCREMENT_EXAMPLES, MAX_NUM_EXAMPLES+1, INCREMENT_EXAMPLES))):\n",
        "  for f, features_size in enumerate(list(range(INCREMENT_FEATURES, MAX_NUM_FEATURES+1, INCREMENT_FEATURES))):\n",
        "    dataset, labels = sklearn.datasets.make_classification(n_samples=examples_size, n_features=features_size, n_classes=2, weights=[0.75])\n",
        "    n_train = round(TRAIN_PROPORTION * np.size(dataset, 0))\n",
        "    data_train = dataset[:n_train]\n",
        "    labels_train =  labels[:n_train]\n",
        "    data_test = dataset[n_train:]\n",
        "    labels_test =  labels[n_train:]\n",
        "\n",
        "    client_dataset_train = create_client_dataset(data_train, labels_train)\n",
        "    client_dataset_test = create_client_dataset(data_test, labels_test)\n",
        "    preprocessed_example_dataset_bench = preprocess(client_dataset_train.create_tf_dataset_for_client(client_dataset_train.client_ids[0]))\n",
        "\n",
        "    # Create TFF interative process.\n",
        "    iterative_process = tff.learning.build_federated_averaging_process(\n",
        "        model_fn=lambda: model_builder(input_dim=features_size, input_spec=preprocessed_example_dataset_bench.element_spec),\n",
        "        client_optimizer_fn=lambda: tf.keras.optimizers.SGD(learning_rate=1.0),\n",
        "        server_optimizer_fn=lambda: tf.keras.optimizers.Nadam(learning_rate=0.5),\n",
        "        use_experimental_simulation_loop = True\n",
        "    )\n",
        "\n",
        "    state = iterative_process.initialize()\n",
        "    tff_model_bench = create_keras_model_benchmark(features_size)\n",
        "    federated_train_data = make_federated_data(client_dataset_train, range(n_train))\n",
        "\n",
        "    start_t = time.process_time()\n",
        "    state, metrics = iterative_process.next(state, federated_train_data)\n",
        "    elapsed_time = time.process_time() - start_t\n",
        "    eval_data[e][f] = elapsed_time\n",
        "    print(examples_size, features_size, elapsed_time)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5EGNKkrhjNf"
      },
      "source": [
        "figure(num=None, figsize=(8, 6), dpi=150, facecolor='w', edgecolor='k')\n",
        "x_axis = list(range(INCREMENT_EXAMPLES, MAX_NUM_EXAMPLES+1, INCREMENT_EXAMPLES))\n",
        "plt.plot(x_axis, eval_data)\n",
        "plt.ylabel('Runtime [s]')\n",
        "plt.xlabel('Number of training clients')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}